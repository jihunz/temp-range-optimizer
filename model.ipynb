{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 열처리 공정 불량률 최소화를 위한 공정별 최적 온도 범위 조합 도출 모듈"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. train.py — XGBoost 학습 로직\n",
    "- **데이터 준비**: `load_dataset`으로 분할을 불러오고 `TargetScaler`로 불량률을 10⁵배 확장하여 수치 안정성 확보\n",
    "- **모델 구성**: `DEFAULT_CONFIG`의 하이퍼파라미터로 `XGBRegressor` 생성, 검증 셋을 `eval_set`으로 등록해 학습 진행 상황을 모니터링\n",
    "- **성능 평가**: 학습/검증(+선택적 테스트) 셋에 대해 MAE·RMSE·R²를 계산하여 JSON으로 저장\n",
    "- **설명 가능성 확보**: XGBoost Booster 혹은 `feature_importances_`를 이용해 gain 기반 피처 중요도를 CSV로 출력\n",
    "- **재현성 관리**: 실행마다 `run_id`를 생성하고 최신 run 마커를 업데이트해 다른 모듈이 동일 run 결과 참조\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py 전체 코드\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from common import (\n",
    "    DatasetSplit,\n",
    "    TargetScaler,\n",
    "    ensure_logger,\n",
    "    ensure_result_directories,\n",
    "    generate_run_id,\n",
    "    load_config,\n",
    "    load_dataset,\n",
    "    write_latest_run_marker,\n",
    ")\n",
    "\n",
    "\n",
    "def _scale_split(split: DatasetSplit, scaler: TargetScaler) -> DatasetSplit:\n",
    "    \"\"\"타깃 스케일링: 불량률이 1e-4 수준이므로 1e5 배 확대 후 학습.\"\"\"\n",
    "\n",
    "    if not scaler.enabled:\n",
    "        return split\n",
    "    return DatasetSplit(\n",
    "        features=split.features,\n",
    "        target=scaler.scale_series(split.target),\n",
    "        lots=split.lots,\n",
    "    )\n",
    "\n",
    "\n",
    "def _evaluate(model: XGBRegressor, split: DatasetSplit, scaler: TargetScaler) -> Dict[str, float]:\n",
    "    \"\"\"예측값을 역스케일하여 MAE/RMSE/R²를 계산.\"\"\"\n",
    "\n",
    "    predictions = model.predict(split.features)\n",
    "    predictions = scaler.inverse_values(predictions)\n",
    "    actual = split.target.to_numpy(dtype=float)\n",
    "    mae = float(mean_absolute_error(actual, predictions))\n",
    "    rmse = float(np.sqrt(mean_squared_error(actual, predictions)))\n",
    "    r2 = float(r2_score(actual, predictions))\n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "\n",
    "def train(\n",
    "    *,\n",
    "    config_path: Optional[Path] = None,\n",
    "    run_id: Optional[str] = None,\n",
    "    model_name: str = \"xgb_baseline\",\n",
    "    save_feature_importances: bool = True,\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"XGBoost 회귀 모델 학습 파이프라인.\"\"\"\n",
    "\n",
    "    logger = ensure_logger()\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    base_result_dir = Path(str(config[\"paths\"][\"result_root\"]))\n",
    "    run_id = run_id or generate_run_id()\n",
    "    run_paths = ensure_result_directories(base_result_dir, config, run_id)\n",
    "\n",
    "    # 1. 데이터 분할 로드\n",
    "    train_split = load_dataset(\"train\", config)\n",
    "    val_split = load_dataset(\"validation\", config)\n",
    "    try:\n",
    "        test_split = load_dataset(\"test\", config)\n",
    "    except FileNotFoundError:\n",
    "        test_split = None\n",
    "\n",
    "    training_cfg = config[\"training\"]\n",
    "    scaler = TargetScaler.from_config(training_cfg.get(\"target_scaler\", {}))\n",
    "\n",
    "    # 2. 타깃 스케일링으로 XGBoost의 손실 스케일을 안정화\n",
    "    scaled_train = _scale_split(train_split, scaler)\n",
    "    scaled_val = _scale_split(val_split, scaler)\n",
    "\n",
    "    # 3. 하이퍼파라미터 로딩 및 기본값 보강\n",
    "    params = dict(training_cfg.get(\"model_params\", {}))\n",
    "    params.setdefault(\"random_state\", training_cfg.get(\"random_seed\", 42))\n",
    "    params.setdefault(\"n_jobs\", training_cfg.get(\"n_jobs\", -1))\n",
    "    eval_metric = training_cfg.get(\"eval_metric\")\n",
    "    if eval_metric and \"eval_metric\" not in params:\n",
    "        params[\"eval_metric\"] = eval_metric  # 예: RMSE\n",
    "\n",
    "    model = XGBRegressor(**params)\n",
    "\n",
    "    # 4. 검증 셋을 eval_set으로 지정 → early stopping/평가에 활용 가능\n",
    "    eval_set = []\n",
    "    fit_kwargs: Dict[str, object] = {}\n",
    "    if val_split.features.shape[0] > 0:\n",
    "        eval_set = [\n",
    "            (scaled_train.features, scaled_train.target.to_numpy(dtype=float)),\n",
    "            (scaled_val.features, scaled_val.target.to_numpy(dtype=float)),\n",
    "        ]\n",
    "        fit_kwargs[\"eval_set\"] = eval_set\n",
    "\n",
    "    # 5. 모델 학습 (조기 종료 옵션을 config에서 제어 가능)\n",
    "    model.fit(\n",
    "        scaled_train.features,\n",
    "        scaled_train.target.to_numpy(dtype=float),\n",
    "        **fit_kwargs,\n",
    "    )\n",
    "\n",
    "    # 6. 원본 스케일에서 성능 지표 산출\n",
    "    metrics: Dict[str, Dict[str, float]] = {\n",
    "        \"train\": _evaluate(model, train_split, scaler),\n",
    "        \"validation\": _evaluate(model, val_split, scaler),\n",
    "    }\n",
    "    if test_split is not None:\n",
    "        metrics[\"test\"] = _evaluate(model, test_split, scaler)\n",
    "\n",
    "    metrics_path = run_paths[\"reports\"] / f\"{model_name}_metrics.json\"\n",
    "    metrics_path.write_text(json.dumps(metrics, indent=2, ensure_ascii=False))\n",
    "\n",
    "    # 7. 모델 저장 (Optuna/SHAP 등 후속 모듈에서 재사용)\n",
    "    model_path = run_paths[\"models\"] / f\"{model_name}.joblib\"\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    # 8. Gain 기반 피처 중요도를 CSV로 내보내 설명 가능성 확보\n",
    "    feature_importance_path: Optional[Path] = None\n",
    "    if save_feature_importances:\n",
    "        try:\n",
    "            booster = model.get_booster()\n",
    "            scores = booster.get_score(importance_type=\"gain\")\n",
    "            records = (\n",
    "                pd.DataFrame(\n",
    "                    sorted(scores.items(), key=lambda item: item[1], reverse=True),\n",
    "                    columns=[\"feature\", \"importance\"],\n",
    "                )\n",
    "                if scores\n",
    "                else pd.DataFrame(columns=[\"feature\", \"importance\"])\n",
    "            )\n",
    "        except Exception:\n",
    "            try:\n",
    "                importances = getattr(model, \"feature_importances_\", None)\n",
    "                if importances is None:\n",
    "                    raise AttributeError\n",
    "                records = pd.DataFrame(\n",
    "                    {\n",
    "                        \"feature\": train_split.features.columns,\n",
    "                        \"importance\": np.asarray(importances, dtype=float),\n",
    "                    }\n",
    "                )\n",
    "            except Exception:\n",
    "                records = pd.DataFrame(columns=[\"feature\", \"importance\"])\n",
    "\n",
    "        feature_importance_path = run_paths[\"reports\"] / f\"{model_name}_feature_importances.csv\"\n",
    "        records.to_csv(feature_importance_path, index=False)\n",
    "\n",
    "    # 9. run 정보 기록 → 다른 모듈이 최신 결과에 접근 가능\n",
    "    write_latest_run_marker(base_result_dir, config, run_id)\n",
    "    logger.info(\"Training completed. Run %s stored at %s\", run_id, run_paths[\"run_dir\"])\n",
    "\n",
    "    result: Dict[str, str] = {\n",
    "        \"run_id\": run_id,\n",
    "        \"model_path\": str(model_path),\n",
    "        \"metrics_path\": str(metrics_path),\n",
    "    }\n",
    "    if feature_importance_path is not None:\n",
    "        result[\"feature_importances_path\"] = str(feature_importance_path)\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. evaluate.py — 모델 성능 검증\n",
    "- **예측값 평가**: 예측값에 대한 MAE/RMSE/R² 평가\n",
    "- **검증 결과 저장**: LOT별 실제/예측값 비교표 및 그래프 저장\n",
    "- **선택적 SHAP 평가 및 저장**: 선택적으로 LOT별 상위 SHAP 피처를 CSV로 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate.py 전체 코드\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "\n",
    "from common import (\n",
    "    TargetScaler,\n",
    "    configure_matplotlib,\n",
    "    ensure_logger,\n",
    "    ensure_result_directories,\n",
    "    load_config,\n",
    "    load_dataset,\n",
    "    read_latest_run_marker,\n",
    ")\n",
    "\n",
    "\n",
    "def _compute_metrics(df: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"LOT별 예측 결과를 바탕으로 MAE/RMSE/R² 계산.\"\"\"\n",
    "\n",
    "    mae = float(df[\"absolute_error\"].mean())\n",
    "    rmse = float(np.sqrt(df[\"squared_error\"].mean()))\n",
    "    actual = df[\"actual_defect_rate\"].values\n",
    "    predicted = df[\"predicted_defect_rate\"].values\n",
    "    ss_res = float(np.sum((actual - predicted) ** 2))\n",
    "    ss_tot = float(np.sum((actual - actual.mean()) ** 2))\n",
    "    r2 = 1.0 - ss_res / ss_tot if ss_tot != 0 else float(\"nan\")\n",
    "    return {\"mae\": mae, \"rmse\": rmse, \"r2\": r2}\n",
    "\n",
    "\n",
    "def _save_comparison_plot(df: pd.DataFrame, destination: Path) -> Path:\n",
    "    \"\"\"실제/예측 불량률을 LOT 순서대로 시각화.\"\"\"\n",
    "\n",
    "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "    sorted_df = df.sort_values(\"actual_defect_rate\")\n",
    "    indices = np.arange(len(sorted_df))\n",
    "    width = 0.35\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(indices - width / 2, sorted_df[\"actual_defect_rate\"], width, label=\"Actual\")\n",
    "    plt.bar(indices + width / 2, sorted_df[\"predicted_defect_rate\"], width, label=\"Predicted\")\n",
    "    plt.xticks(indices, sorted_df[\"LOT_NO\"], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(\"Defect rate\")\n",
    "    plt.title(\"LOT-level actual vs predicted defect rate\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(destination, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "    return destination\n",
    "\n",
    "\n",
    "def _save_lot_shap_summary(\n",
    "    model,\n",
    "    scaler: TargetScaler,\n",
    "    dataset,\n",
    "    destination: Path,\n",
    "    top_k: int = 5,\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"LOT별 평균 |SHAP| Top-k 피처를 CSV로 저장.\"\"\"\n",
    "\n",
    "    explainer = shap.TreeExplainer(model, feature_perturbation=\"tree_path_dependent\")\n",
    "    shap_values = explainer(dataset.features).values\n",
    "    shap_values = scaler.inverse_values(shap_values)\n",
    "    shap_abs = np.abs(shap_values)\n",
    "    shap_df = pd.DataFrame(shap_abs, columns=dataset.features.columns)\n",
    "    shap_df[\"LOT_NO\"] = dataset.lots.values\n",
    "\n",
    "    grouped = shap_df.groupby(\"LOT_NO\").mean()\n",
    "    records = []\n",
    "    for lot_id, row in grouped.iterrows():\n",
    "        top_features = row.sort_values(ascending=False).head(top_k)\n",
    "        for rank, (feature, value) in enumerate(top_features.items(), start=1):\n",
    "            records.append(\n",
    "                {\n",
    "                    \"LOT_NO\": lot_id,\n",
    "                    \"rank\": rank,\n",
    "                    \"feature\": feature,\n",
    "                    \"mean_abs_shap\": float(value),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not records:\n",
    "        return None\n",
    "\n",
    "    summary_df = pd.DataFrame(records)\n",
    "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "    summary_df.to_csv(destination, index=False)\n",
    "    return destination\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    *,\n",
    "    config_path: Optional[Path] = None,\n",
    "    run_id: Optional[str] = None,\n",
    "    model_name: str = \"xgb_baseline\",\n",
    "    split: str = \"validation\",\n",
    "    compute_shap: bool = True,\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"LOT 단위 평가 및 리포트 작성.\"\"\"\n",
    "\n",
    "    ensure_logger()\n",
    "    configure_matplotlib()\n",
    "    config = load_config(config_path)\n",
    "    base_result_dir = Path(str(config[\"paths\"][\"result_root\"]))\n",
    "\n",
    "    if run_id is None:\n",
    "        run_id = read_latest_run_marker(base_result_dir, config)\n",
    "        if run_id is None:\n",
    "            raise ValueError(\"No recorded run. Train the model before running evaluation.\")\n",
    "\n",
    "    run_paths = ensure_result_directories(base_result_dir, config, run_id)\n",
    "    model_path = run_paths[\"models\"] / f\"{model_name}.joblib\"\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    dataset = load_dataset(split, config)\n",
    "    scaler = TargetScaler.from_config(config[\"training\"].get(\"target_scaler\", {}))\n",
    "\n",
    "    predictions = scaler.inverse_values(model.predict(dataset.features))\n",
    "    report_df = pd.DataFrame(\n",
    "        {\n",
    "            \"LOT_NO\": dataset.lots.values,\n",
    "            \"actual_defect_rate\": dataset.target.values,\n",
    "            \"predicted_defect_rate\": predictions,\n",
    "        }\n",
    "    )\n",
    "    report_df[\"absolute_error\"] = np.abs(\n",
    "        report_df[\"actual_defect_rate\"] - report_df[\"predicted_defect_rate\"]\n",
    "    )\n",
    "    report_df[\"squared_error\"] = (\n",
    "        report_df[\"actual_defect_rate\"] - report_df[\"predicted_defect_rate\"]\n",
    "    ) ** 2\n",
    "\n",
    "    metrics = _compute_metrics(report_df)\n",
    "\n",
    "    reports_dir = run_paths[\"reports\"]\n",
    "    plots_dir = run_paths[\"plots\"]\n",
    "\n",
    "    report_path = reports_dir / f\"{model_name}_{split}_lot_evaluation.csv\"\n",
    "    metrics_path = reports_dir / f\"{model_name}_{split}_lot_metrics.json\"\n",
    "    plot_path = plots_dir / f\"{model_name}_{split}_lot_comparison.png\"\n",
    "\n",
    "    report_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    report_df.to_csv(report_path, index=False)\n",
    "    metrics_path.write_text(json.dumps(metrics, indent=2, ensure_ascii=False))\n",
    "    _save_comparison_plot(report_df, plot_path)\n",
    "\n",
    "    shap_summary_path: Optional[Path] = None\n",
    "    if compute_shap:\n",
    "        shap_summary_path = _save_lot_shap_summary(\n",
    "            model,\n",
    "            scaler,\n",
    "            dataset,\n",
    "            reports_dir / f\"{model_name}_{split}_lot_shap_summary.csv\",\n",
    "        )\n",
    "\n",
    "    logging.info(\"Evaluation completed for run %s\", run_id)\n",
    "\n",
    "    result: Dict[str, Optional[str]] = {\n",
    "        \"run_id\": run_id,\n",
    "        \"report_path\": str(report_path),\n",
    "        \"metrics_path\": str(metrics_path),\n",
    "        \"plot_path\": str(plot_path),\n",
    "    }\n",
    "    if shap_summary_path:\n",
    "        result[\"shap_summary_path\"] = str(shap_summary_path)\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. analyze.py — SHAP 기반 심층 분석 상세 설명\n",
    "- **샘플링 전략**: `max_samples` 설정으로 분석 비용을 제어하며, LOT 레이블을 유지한 상태로 샘플링\n",
    "- **SHAP 계산**: `TreeExplainer`로 절대 SHAP·상호작용 값을 계산하고, 스케일러로 원 단위로 복원\n",
    "- **전역/지역 시각화**: 요약 플롯, 의존성 플롯, LOT별 바 차트, 상호작용 히트맵을 생성해 영향도를 다면적으로 확인\n",
    "- **표면 분석**: 상위 2개 피처를 활용해 예측 표면을 생성, 온도 조합에 따른 불량률 변화를 3D로 탐색\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze.py 전체 코드\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "import shap\n",
    "\n",
    "from common import (\n",
    "    DatasetSplit,\n",
    "    TargetScaler,\n",
    "    compute_feature_ranges,\n",
    "    configure_matplotlib,\n",
    "    ensure_logger,\n",
    "    ensure_result_directories,\n",
    "    load_config,\n",
    "    load_dataset,\n",
    "    mean_absolute_shap,\n",
    "    read_latest_run_marker,\n",
    ")\n",
    "\n",
    "\n",
    "def _sample_dataset(split: DatasetSplit, max_samples: Optional[int], random_state: int) -> DatasetSplit:\n",
    "    \"\"\"SHAP 비용을 줄이기 위해 LOT 라벨을 유지한 채 일부 행만 추출.\"\"\"\n",
    "\n",
    "    if max_samples is None or len(split.features) <= max_samples:\n",
    "        return split\n",
    "    sampled_features = split.features.sample(n=max_samples, random_state=random_state)\n",
    "    sampled_target = split.target.loc[sampled_features.index]\n",
    "    sampled_lots = split.lots.loc[sampled_features.index]\n",
    "    return DatasetSplit(\n",
    "        features=sampled_features.reset_index(drop=True),\n",
    "        target=sampled_target.reset_index(drop=True),\n",
    "        lots=sampled_lots.reset_index(drop=True),\n",
    "    )\n",
    "\n",
    "\n",
    "def _select_top_features(columns: Sequence[str], shap_values: np.ndarray, top_k: int) -> List[str]:\n",
    "    \"\"\"평균 절대 SHAP 순으로 상위 피처 추출.\"\"\"\n",
    "\n",
    "    scores = mean_absolute_shap(shap_values)\n",
    "    order = np.argsort(scores)[::-1]\n",
    "    limit = min(top_k, len(columns))\n",
    "    return [columns[idx] for idx in order[:limit]]\n",
    "\n",
    "\n",
    "def _save_summary_plot(\n",
    "    explanation: shap.Explanation,\n",
    "    shap_values: np.ndarray,\n",
    "    destination: Path,\n",
    "    top_k: int,\n",
    ") -> Path:\n",
    "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shap.summary_plot(shap_values, explanation.data, show=False, max_display=top_k)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(destination, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "    return destination\n",
    "\n",
    "\n",
    "def _save_summary_table(columns: Sequence[str], shap_values: np.ndarray, destination: Path) -> Path:\n",
    "    \"\"\"전역 중요도를 CSV로 남겨 수치 비교 가능.\"\"\"\n",
    "\n",
    "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "    mean_abs = mean_absolute_shap(shap_values)\n",
    "    df = pd.DataFrame({\"feature\": columns, \"mean_abs_shap\": mean_abs})\n",
    "    df.sort_values(\"mean_abs_shap\", ascending=False, inplace=True)\n",
    "    df.to_csv(destination, index=False)\n",
    "    return destination\n",
    "\n",
    "\n",
    "def _save_dependence_plots(\n",
    "    shap_values: np.ndarray,\n",
    "    feature_matrix: pd.DataFrame,\n",
    "    features: Sequence[str],\n",
    "    destination_dir: Path,\n",
    ") -> List[Path]:\n",
    "    \"\"\"단일 피처와 목표 간 관계를 시각화.\"\"\"\n",
    "\n",
    "    destination_dir.mkdir(parents=True, exist_ok=True)\n",
    "    paths: List[Path] = []\n",
    "    for feature in features:\n",
    "        shap.dependence_plot(\n",
    "            feature,\n",
    "            shap_values,\n",
    "            feature_matrix,\n",
    "            show=False,\n",
    "            feature_names=list(feature_matrix.columns),\n",
    "        )\n",
    "        plt.tight_layout()\n",
    "        path = destination_dir / f\"dependence_{feature}.png\"\n",
    "        plt.savefig(path, bbox_inches=\"tight\", dpi=300)\n",
    "        plt.close()\n",
    "        paths.append(path)\n",
    "    return paths\n",
    "\n",
    "\n",
    "def _save_interaction_heatmap(\n",
    "    feature_names: Sequence[str],\n",
    "    interaction_values: np.ndarray,\n",
    "    destination: Path,\n",
    "    top_k: Optional[int],\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"SHAP 상호작용을 평균 절대값으로 요약해 히트맵으로 표현.\"\"\"\n",
    "\n",
    "    if interaction_values is None:\n",
    "        return None\n",
    "\n",
    "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    strength = np.mean(np.abs(interaction_values), axis=0)\n",
    "    np.fill_diagonal(strength, 0.0)\n",
    "    interaction_df = pd.DataFrame(strength, index=feature_names, columns=feature_names)\n",
    "\n",
    "    if top_k is not None and top_k < len(feature_names):\n",
    "        totals = interaction_df.abs().sum(axis=0).sort_values(ascending=False)\n",
    "        selected = totals.head(top_k).index\n",
    "        interaction_df = interaction_df.loc[selected, selected]\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(interaction_df, cmap=\"coolwarm\", center=0.0)\n",
    "    plt.title(\"Mean |SHAP interaction|\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(destination, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "    return destination\n",
    "\n",
    "\n",
    "def _save_lot_barplots(\n",
    "    shap_values: np.ndarray,\n",
    "    columns: Sequence[str],\n",
    "    lots: Sequence[str],\n",
    "    destination: Path,\n",
    "    top_k_features: int,\n",
    "    top_k_lots: int,\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"LOT별 평균 절대 SHAP 상위 피처를 막대로 표현.\"\"\"\n",
    "\n",
    "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "    shap_df = pd.DataFrame(np.abs(shap_values), columns=columns)\n",
    "    shap_df[\"LOT_NO\"] = list(lots)\n",
    "    lot_mean = shap_df.groupby(\"LOT_NO\").mean()\n",
    "    if lot_mean.empty:\n",
    "        return None\n",
    "\n",
    "    lot_scores = lot_mean.sum(axis=1).sort_values(ascending=False)\n",
    "    selected_lots = lot_scores.head(top_k_lots).index\n",
    "    if len(selected_lots) == 0:\n",
    "        return None\n",
    "\n",
    "    rows = len(selected_lots)\n",
    "    fig, axes = plt.subplots(rows, 1, figsize=(12, 4 * rows))\n",
    "    if rows == 1:\n",
    "        axes = [axes]\n",
    "    for ax, lot in zip(axes, selected_lots):\n",
    "        values = lot_mean.loc[lot].sort_values(ascending=False).head(top_k_features)\n",
    "        ax.barh(list(values.index[::-1]), list(values.values[::-1]))\n",
    "        ax.set_title(f\"LOT {lot} | Top SHAP contributions\")\n",
    "        ax.set_xlabel(\"Mean |SHAP|\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(destination, bbox_inches=\"tight\", dpi=300)\n",
    "    plt.close()\n",
    "    return destination\n",
    "\n",
    "\n",
    "def _save_surface_plot(\n",
    "    model,\n",
    "    scaler: TargetScaler,\n",
    "    columns: Sequence[str],\n",
    "    feature_pair: Tuple[str, str],\n",
    "    feature_ranges: Dict[str, Tuple[float, float]],\n",
    "    baseline_row: pd.Series,\n",
    "    destination: Path,\n",
    ") -> Optional[Path]:\n",
    "    \"\"\"상위 2개 피처의 조합에 따른 예측 불량률을 3D 표면으로 저장.\"\"\"\n",
    "\n",
    "    a, b = feature_pair\n",
    "    if a not in feature_ranges or b not in feature_ranges:\n",
    "        return None\n",
    "\n",
    "    destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "    range_a = feature_ranges[a]\n",
    "    range_b = feature_ranges[b]\n",
    "    grid_a = np.linspace(range_a[0], range_a[1], 40)\n",
    "    grid_b = np.linspace(range_b[0], range_b[1], 40)\n",
    "    mesh_a, mesh_b = np.meshgrid(grid_a, grid_b)\n",
    "\n",
    "    repeated = pd.DataFrame(\n",
    "        np.repeat(baseline_row.to_frame().T.values, mesh_a.size, axis=0),\n",
    "        columns=columns,\n",
    "    )\n",
    "    repeated[a] = mesh_a.ravel()\n",
    "    repeated[b] = mesh_b.ravel()\n",
    "\n",
    "    predictions = model.predict(repeated)\n",
    "    surface = scaler.inverse_values(predictions).reshape(mesh_a.shape)\n",
    "\n",
    "    figure = go.Figure(\n",
    "        data=[\n",
    "            go.Surface(x=grid_a, y=grid_b, z=surface, colorscale=\"Viridis\", showscale=True)\n",
    "        ]\n",
    "    )\n",
    "    figure.update_layout(\n",
    "        title=f\"Predicted defect rate surface ({a} vs {b})\",\n",
    "        scene=dict(xaxis_title=a, yaxis_title=b, zaxis_title=\"Predicted defect rate\"),\n",
    "    )\n",
    "    figure.write_html(destination)\n",
    "    return destination\n",
    "\n",
    "\n",
    "def analyze(\n",
    "    *,\n",
    "    config_path: Optional[Path] = None,\n",
    "    run_id: Optional[str] = None,\n",
    "    model_name: str = \"xgb_baseline\",\n",
    "    split: str = \"train\",\n",
    ") -> Dict[str, Optional[str]]:\n",
    "    \"\"\"SHAP 기반 분석 artefact를 생성하고 경로를 반환.\"\"\"\n",
    "\n",
    "    ensure_logger()\n",
    "    configure_matplotlib()\n",
    "    config = load_config(config_path)\n",
    "\n",
    "    base_result_dir = Path(str(config[\"paths\"][\"result_root\"]))\n",
    "    if run_id is None:\n",
    "        run_id = read_latest_run_marker(base_result_dir, config)\n",
    "        if run_id is None:\n",
    "            raise ValueError(\"No recorded run. Execute training first or provide run_id explicitly.\")\n",
    "\n",
    "    run_paths = ensure_result_directories(base_result_dir, config, run_id)\n",
    "    model_path = run_paths[\"models\"] / f\"{model_name}.joblib\"\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
    "\n",
    "    dataset = load_dataset(split, config)\n",
    "    shap_cfg = config[\"shap\"]\n",
    "    sampled_dataset = _sample_dataset(\n",
    "        dataset,\n",
    "        shap_cfg.get(\"max_samples\"),\n",
    "        random_state=config[\"training\"].get(\"random_seed\", 42),\n",
    "    )\n",
    "\n",
    "    scaler = TargetScaler.from_config(config[\"training\"].get(\"target_scaler\", {}))\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # 1. 단일/상호작용 SHAP 값을 계산하고 실제 불량률 단위로 복원\n",
    "    explainer = shap.TreeExplainer(model, feature_perturbation=\"tree_path_dependent\")\n",
    "    explanation = explainer(sampled_dataset.features)\n",
    "    shap_values = scaler.inverse_values(explanation.values)\n",
    "    interaction_values = None\n",
    "    if shap_cfg.get(\"interaction_top_k\", 0) > 0:\n",
    "        raw_interactions = explainer.shap_interaction_values(sampled_dataset.features)\n",
    "        interaction_values = scaler.inverse_values(raw_interactions)\n",
    "\n",
    "    columns = list(sampled_dataset.features.columns)\n",
    "    top_features = _select_top_features(\n",
    "        columns,\n",
    "        shap_values,\n",
    "        top_k=shap_cfg.get(\"top_k_features\", 15),\n",
    "    )\n",
    "\n",
    "    # 2. 전역 요약 그래프/표 저장\n",
    "    summary_plot = _save_summary_plot(\n",
    "        explanation,\n",
    "        shap_values,\n",
    "        run_paths[\"shap\"] / f\"{model_name}_summary.png\",\n",
    "        top_k=shap_cfg.get(\"top_k_features\", 15),\n",
    "    )\n",
    "\n",
    "    summary_table = _save_summary_table(\n",
    "        columns,\n",
    "        shap_values,\n",
    "        run_paths[\"reports\"] / f\"{model_name}_shap_summary.csv\",\n",
    "    )\n",
    "\n",
    "    # 3. 관심 피처에 대한 의존성 플롯 및 LOT별 막대 플롯 생성\n",
    "    dependence_features = top_features[: shap_cfg.get(\"dependence_max_features\", 5)]\n",
    "    dependence_paths = _save_dependence_plots(\n",
    "        shap_values,\n",
    "        sampled_dataset.features,\n",
    "        dependence_features,\n",
    "        run_paths[\"shap\"] / \"dependence\",\n",
    "    )\n",
    "\n",
    "    heatmap_path = _save_interaction_heatmap(\n",
    "        columns,\n",
    "        interaction_values,\n",
    "        run_paths[\"shap\"] / f\"{model_name}_interaction_heatmap.png\",\n",
    "        top_k=shap_cfg.get(\"interaction_top_k\"),\n",
    "    )\n",
    "\n",
    "    lot_barplot_path = _save_lot_barplots(\n",
    "        shap_values,\n",
    "        columns,\n",
    "        sampled_dataset.lots,\n",
    "        run_paths[\"shap\"] / f\"{model_name}_lot_shap.png\",\n",
    "        top_k_features=shap_cfg.get(\"lot_top_features\", 10),\n",
    "        top_k_lots=shap_cfg.get(\"lot_top_lots\", 5),\n",
    "    )\n",
    "\n",
    "    # 4. 상위 2개 피처의 영향도를 3D 표면으로 시각화\n",
    "    surface_path: Optional[Path] = None\n",
    "    if len(top_features) >= 2:\n",
    "        feature_ranges = compute_feature_ranges(dataset, top_features[:2])\n",
    "        baseline_row = dataset.features.median()\n",
    "        surface_path = _save_surface_plot(\n",
    "            model,\n",
    "            scaler,\n",
    "            columns,\n",
    "            (top_features[0], top_features[1]),\n",
    "            feature_ranges,\n",
    "            baseline_row,\n",
    "            run_paths[\"shap\"] / f\"{model_name}_surface.html\",\n",
    "        )\n",
    "\n",
    "    logging.info(\"SHAP analysis completed for run %s\", run_id)\n",
    "\n",
    "    artefacts: Dict[str, Optional[str]] = {\n",
    "        \"run_id\": run_id,\n",
    "        \"summary_plot\": str(summary_plot),\n",
    "        \"summary_table\": str(summary_table),\n",
    "    }\n",
    "    if dependence_paths:\n",
    "        artefacts[\"dependence_plots\"] = json.dumps([str(path) for path in dependence_paths], ensure_ascii=False)\n",
    "    if heatmap_path:\n",
    "        artefacts[\"interaction_heatmap\"] = str(heatmap_path)\n",
    "    if lot_barplot_path:\n",
    "        artefacts[\"lot_barplot\"] = str(lot_barplot_path)\n",
    "    if surface_path:\n",
    "        artefacts[\"surface_plot\"] = str(surface_path)\n",
    "    return artefacts\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. optimize.py — Bayesian optimization 기반 불량률 최소화를 위한 설비별 최적 온도 평균, 표준 편차 도출\n",
    "- **탐색 대상 선정**: LOT 필터링, `target_temperature_features` 교차 검증으로 실제 존재하는 컬럼만 대상으로 설정\n",
    "- **탐색 공간 구성**: 학습 데이터에서 온도 피처의 최소/최대값을 계산해 안전한 탐색 범위를 확보\n",
    "- **Optuna 목적 함수**: 각 시도에서 피처 값을 조정해 모델 재예측 후 역스케일하여 불량률을 최소화\n",
    "- **성과 기록**: LOT별 baseline/최적 예측, 개선 폭, 추천 파라미터를 CSV로 저장해 현장 적용 근거로 활용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize.py 전체 코드\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Tuple\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "from common import (\n",
    "    TargetScaler,\n",
    "    compute_feature_ranges,\n",
    "    ensure_logger,\n",
    "    ensure_result_directories,\n",
    "    list_temperature_features,\n",
    "    load_config,\n",
    "    load_dataset,\n",
    "    read_latest_run_marker,\n",
    ")\n",
    "\n",
    "\n",
    "def _select_lots(series: pd.Series, lot_ids: Optional[Sequence[str]], max_lots: Optional[int]) -> List[str]:\n",
    "    \"\"\"LOT 필터링: 지정 목록 우선, 없으면 등장 순서대로 중복 없이 선택.\"\"\"\n",
    "\n",
    "    if lot_ids:\n",
    "        selected = [lot for lot in lot_ids if lot in set(series.values)]\n",
    "    else:\n",
    "        # 순서를 보존하면서 중복 제거 → 생산 순서를 유지한 채 탐색 가능\n",
    "        seen = set()\n",
    "        selected = []\n",
    "        for value in series.values:\n",
    "            if value in seen:\n",
    "                continue\n",
    "            seen.add(value)\n",
    "            selected.append(value)\n",
    "    if max_lots is not None:\n",
    "        selected = selected[: int(max_lots)]\n",
    "    return selected\n",
    "\n",
    "\n",
    "def _optimize_single_lot(\n",
    "    model,\n",
    "    scaler: TargetScaler,\n",
    "    base_row: pd.Series,\n",
    "    feature_ranges: Dict[str, Tuple[float, float]],\n",
    "    opt_config: Dict[str, object],\n",
    ") -> Tuple[Dict[str, float], float]:\n",
    "    \"\"\"단일 LOT을 대상으로 TPE 기반 Optuna 탐색을 수행.\"\"\"\n",
    "\n",
    "    sampler = optuna.samplers.TPESampler(seed=opt_config.get(\"seed\", 2024))\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "\n",
    "    def objective(trial: optuna.trial.Trial) -> float:\n",
    "        # 후보 행을 복사한 뒤 탐색 대상 피처만 업데이트\n",
    "        candidate = base_row.to_frame().T.copy()\n",
    "        for feature_name, bounds in feature_ranges.items():\n",
    "            low, high = bounds\n",
    "            value = trial.suggest_float(feature_name, low, high)\n",
    "            candidate.at[0, feature_name] = value\n",
    "        prediction = float(model.predict(candidate)[0])\n",
    "        return scaler.inverse_scalar(prediction)  # 불량률 단위로 평가\n",
    "\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=int(opt_config.get(\"n_trials\", 60)),\n",
    "        timeout=opt_config.get(\"timeout_seconds\"),\n",
    "        n_jobs=opt_config.get(\"n_jobs\", 1),\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "    best_params = {key: float(value) for key, value in study.best_params.items()}\n",
    "    best_value = float(study.best_value)\n",
    "    return best_params, best_value\n",
    "\n",
    "\n",
    "def optimize(\n",
    "    *,\n",
    "    config_path: Optional[Path] = None,\n",
    "    run_id: Optional[str] = None,\n",
    "    model_name: str = \"xgb_baseline\",\n",
    "    split: str = \"validation\",\n",
    "    lot_ids: Optional[Sequence[str]] = None,\n",
    "    max_lots: Optional[int] = None,\n",
    ") -> Path:\n",
    "    \"\"\"Optuna 기반 온도 최적화 전체 파이프라인.\"\"\"\n",
    "\n",
    "    ensure_logger()\n",
    "    config = load_config(config_path)\n",
    "    base_result_dir = Path(str(config[\"paths\"][\"result_root\"]))\n",
    "\n",
    "    if run_id is None:\n",
    "        run_id = read_latest_run_marker(base_result_dir, config)\n",
    "        if run_id is None:\n",
    "            raise ValueError(\"No recorded run. Train the model before running optimization.\")\n",
    "\n",
    "    run_paths = ensure_result_directories(base_result_dir, config, run_id)\n",
    "    model_path = run_paths[\"models\"] / f\"{model_name}.joblib\"\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(f\"Model not found at {model_path}\")\n",
    "\n",
    "    model = joblib.load(model_path)\n",
    "    scaler = TargetScaler.from_config(config[\"training\"].get(\"target_scaler\", {}))\n",
    "\n",
    "    dataset = load_dataset(split, config)\n",
    "    train_split = load_dataset(\"train\", config)\n",
    "\n",
    "    temperature_features = list_temperature_features(dataset.features.columns, config)\n",
    "\n",
    "    opt_config = config[\"optimization\"]\n",
    "    target_feature_list = opt_config.get(\"target_temperature_features\")\n",
    "    candidate_features: Sequence[str]\n",
    "    if target_feature_list:\n",
    "        candidate_features = [\n",
    "            feature for feature in target_feature_list if feature in dataset.features.columns\n",
    "        ]\n",
    "    else:\n",
    "        max_feature_setting = opt_config.get(\"max_temperature_features\")\n",
    "        if max_feature_setting is None:\n",
    "            candidate_features = temperature_features\n",
    "        else:\n",
    "            candidate_features = temperature_features[: int(max_feature_setting)]\n",
    "    if not candidate_features:\n",
    "        raise ValueError(\"No temperature features available for optimization.\")\n",
    "\n",
    "    # 학습 데이터의 범위를 사용해 탐색 공간을 안전하게 제한\n",
    "    feature_ranges = compute_feature_ranges(train_split, candidate_features)\n",
    "    missing = [feature for feature in candidate_features if feature not in feature_ranges]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing feature ranges for: {missing}\")\n",
    "\n",
    "    if max_lots is None:\n",
    "        max_lots_setting = opt_config.get(\"max_lots\")\n",
    "        if max_lots_setting is not None:\n",
    "            max_lots = int(max_lots_setting)\n",
    "\n",
    "    selected_lots = _select_lots(dataset.lots, lot_ids, max_lots)\n",
    "    if not selected_lots:\n",
    "        raise ValueError(\"No lots selected for optimization.\")\n",
    "\n",
    "    records: List[Dict[str, float]] = []\n",
    "    for lot in selected_lots:\n",
    "        mask = dataset.lots == lot\n",
    "        if not mask.any():\n",
    "            continue\n",
    "        base_row = dataset.features.loc[mask].iloc[0].copy()\n",
    "        baseline_prediction = float(model.predict(base_row.to_frame().T)[0])\n",
    "        baseline_prediction = scaler.inverse_scalar(baseline_prediction)\n",
    "\n",
    "        # LOT 단위 최적 탐색 수행\n",
    "        best_params, best_value = _optimize_single_lot(\n",
    "            model,\n",
    "            scaler,\n",
    "            base_row,\n",
    "            feature_ranges,\n",
    "            config[\"optimization\"],\n",
    "        )\n",
    "\n",
    "        record: Dict[str, float] = {\n",
    "            \"LOT_NO\": lot,\n",
    "            \"predicted_defect_rate\": float(best_value),\n",
    "            \"baseline_prediction\": float(baseline_prediction),\n",
    "            \"improvement\": float(baseline_prediction - best_value),\n",
    "        }\n",
    "        record.update(best_params)\n",
    "        records.append(record)\n",
    "        logging.info(\n",
    "            \"Optimized lot %s: baseline=%.6f, optimized=%.6f\",\n",
    "            lot,\n",
    "            baseline_prediction,\n",
    "            best_value,\n",
    "        )\n",
    "\n",
    "    # LOT별 추천 값을 CSV로 저장해 후속 공정 검토에 활용\n",
    "    output_path = run_paths[\"optimization\"] / f\"{model_name}_{split}_optimization.csv\"\n",
    "    result_df = pd.DataFrame(records)\n",
    "    if not result_df.empty:\n",
    "        result_df.sort_values(\"predicted_defect_rate\", inplace=True)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    result_df.to_csv(output_path, index=False)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    optimize()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
